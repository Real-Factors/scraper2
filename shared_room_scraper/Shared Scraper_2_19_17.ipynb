{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "import importlib\n",
    "import urllib\n",
    "import unicodecsv as csv\n",
    "from lxml import html\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "#import syslog\n",
    "#import psycopg2\n",
    "#import shutil\n",
    "#import os\n",
    "#import glob\n",
    "#import subprocess\n",
    "#import time\n",
    "#import sys\n",
    "#from requests.auth import HTTPProxyAuth\n",
    "#from __future__ import division\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try to run this with San Francisco\n",
    "DOMAINS = ['http://sfbay.craigslist.org/search/roo'] \n",
    "\n",
    "#Craigslist doesn't use time zones in its timestamps, so these cutoffs will be\n",
    "#interpreted relative to the local time at the listing location. For example, dt.now()\n",
    "#run from a machine in San Francisco will match listings from 3 hours ago in Boston.\n",
    "LATEST_TS = dt.now()\n",
    "EARLIEST_TS = LATEST_TS - timedelta(hours=.5)\n",
    "\n",
    "\n",
    "OUT_DIR =\"C:\\\\Users\\\\james\\\\Documents\\\\Berkeley_Docs\\\\Spring_17_Courses\\\\CP290 Data Lab\\\\scraper_output\\\\\" #James's directory\n",
    "\n",
    "#OUT_DIR ='/Users/anniedbr/Desktop/CSV/'  #Annie's directory\n",
    "\n",
    "FNAME_BASE = 'data'  # filename prefix for saved data\n",
    "FNAME_TS = True  # append timestamp to filename\n",
    "\n",
    "S3_UPLOAD = False\n",
    "S3_BUCKET = 'scraper2'\n",
    "\n",
    "class RentalListingScraper(object):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            domains = DOMAINS,\n",
    "            earliest_ts = EARLIEST_TS,\n",
    "            latest_ts = LATEST_TS, \n",
    "            out_dir = OUT_DIR,\n",
    "            fname_base = FNAME_BASE,\n",
    "            fname_ts = FNAME_TS,\n",
    "            s3_upload = S3_UPLOAD,\n",
    "            s3_bucket = S3_BUCKET):\n",
    "        \n",
    "        self.domains = domains\n",
    "        self.earliest_ts = earliest_ts\n",
    "        self.latest_ts = latest_ts\n",
    "        self.out_dir = out_dir\n",
    "        self.fname_base = fname_base\n",
    "        self.fname_ts = fname_ts\n",
    "        self.s3_upload = s3_upload\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.ts = dt.now().strftime('%Y%m%d-%H%M%S')  # Use timestamp as file id\n",
    "        #self.ts = fname_ts\n",
    "\n",
    "        log_fname = self.out_dir + self.fname_base \\\n",
    "                + (self.ts if self.fname_ts else '') + '.log'\n",
    "        \n",
    "        importlib.reload(logging)\n",
    "        \n",
    "        logging.basicConfig(filename=log_fname, level=logging.INFO)\n",
    "       \n",
    "        #Suppress info messages from the 'requests' library\n",
    "        #logging.getLogger('requests').setLevel(logging.WARNING)  \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    def _get_str(self, list):\n",
    "        '''\n",
    "        The xpath() function returns a list of items that may be empty. Most of the time,\n",
    "        we want the first of any strings that match the xml query. This helper function\n",
    "        returns that string, or null if the list is empty.\n",
    "        '''\n",
    "        \n",
    "        if len(list) > 0:\n",
    "            return list[0]\n",
    "\n",
    "        return ''\n",
    "    \n",
    "        \n",
    "    def _get_int_prefix(self, str, label):\n",
    "        '''\n",
    "        Bedrooms and square footage have the format \"xx 1br xx 450ft xx\". This helper \n",
    "        function extracts relevant integers from strings of this format.\n",
    "        '''     \n",
    "        \n",
    "        for s in str.split(' '):\n",
    "            if label in s:\n",
    "                return s.strip(label)\n",
    "                \n",
    "        return 0\n",
    "\n",
    "\n",
    "    def _toFloat(self, string_value):\n",
    "        string_value = string_value.strip()\n",
    "        return np.float(string_value) if string_value else np.nan\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "    def _parseListing(self, item):\n",
    "        '''\n",
    "        Note that xpath() returns a list with elements of varying types depending on the\n",
    "        query results: xml objects, strings, etc.\n",
    "        '''\n",
    "        pid = item.xpath('@data-pid')[0]  # post id, always present\n",
    "        info = item.xpath('p[@class=\"result-info\"]')[0]\n",
    "        dt = info.xpath('time/@datetime')[0]\n",
    "        url = info.xpath('a/@href')[0]\n",
    "        if type(info.xpath('a/text()')) == str:\n",
    "            title = info.xpath('a/text()')\n",
    "        else:\n",
    "            title = info.xpath('a/text()')[0]\n",
    "        price = self._get_str(info.xpath('span[@class=\"result-meta\"]/span[@class=\"result-price\"]/text()')).strip('$')\n",
    "        neighb_raw = info.xpath('span[@class=\"result-meta\"]/span[@class=\"result-hood\"]/text()')\n",
    "        if len(neighb_raw) == 0:\n",
    "            neighb = ''\n",
    "        else:\n",
    "            neighb = neighb_raw[0].strip(\" \").strip(\"(\").strip(\")\")\n",
    "        housing_raw = info.xpath('span[@class=\"result-meta\"]/span[@class=\"housing\"]/text()')\n",
    "        if len(housing_raw) == 0:\n",
    "            #beds = 0\n",
    "            sqft = 0\n",
    "        else:\n",
    "            bedsqft = housing_raw[0]\n",
    "            #beds = self._get_int_prefix(bedsqft, \"br\")  # appears as \"1br\" to \"8br\" or missing\n",
    "            sqft = self._get_int_prefix(bedsqft, \"ft\")  # appears as \"000ft\" or missing\n",
    "        #for domain in self.domains:\n",
    "            #url = domain.split('/search')[0] + info.xpath('a/@href')[0]\n",
    "        #return [pid, dt, url, title, price, neighb, beds, sqft]\n",
    "        return [pid, dt, url, title, price, neighb, sqft]\n",
    "\n",
    "    \n",
    "\n",
    "    def _parseAddress(self, tree):\n",
    "        '''\n",
    "        Some listings include an address, but we have to parse it out of an encoded\n",
    "        Google Maps url.\n",
    "        '''\n",
    "        url = self._get_str(tree.xpath('//p[@class=\"mapaddress\"]/small/a/@href'))\n",
    "        \n",
    "        if '?q=loc' not in url:\n",
    "            # That string precedes an address search\n",
    "            return ''\n",
    "            \n",
    "        return urllib.unquote_plus(url.split('?q=loc')[1]).strip(' :')\n",
    "    \n",
    "    #def PageBodyText(self, session, url, proxy=True):\n",
    "    #We've tried section, div\n",
    "    \n",
    "    def PageBodyText(self, url):\n",
    "        \n",
    "        page = requests.get(url)        \n",
    "        tree = html.fromstring(page.content)\n",
    "        path = tree.xpath('//section[@id=\"postingbody\"]')[0]\n",
    "       \n",
    "#         if type(path.xpath(\"text()\")) == str:\n",
    "#             body_text = path.xpath(\"text()\")\n",
    "#         else:\n",
    "#             body_text = path.xpath(\"text()\")[0]\n",
    "#             #body_text = tree.xpath('//section[@id=\"postingbody\"]/text()')[0]   \n",
    "        \n",
    "        body_text = path.xpath('text()')\n",
    "        \n",
    "        return [body_text]\n",
    "    \n",
    "# OLD ATTEMPT\n",
    "\n",
    "#         #if type(tree.xpath('//div[@class=\"print-information print-qrcode-container\"]/text()'))==str:\n",
    "#         if type(path.xpath('text()'))==str:\n",
    "#             body_text = path.xpath('text()')\n",
    "#         else:\n",
    "#             body_text = path.xpath('text()')[0]\n",
    "#             #body_text = tree.xpath('//section[@id=\"postingbody\"]/text()')[0]   \n",
    "#         return [body_text]\n",
    "\n",
    "    \n",
    "    #def PageAttributes(self, session, url, proxy=True):\n",
    "    def PageAttributes(self, url):   \n",
    "        #s = session\n",
    "         \n",
    "        page = requests.get(url, timeout=30)        \n",
    "        tree = html.fromstring(page.content)\n",
    "        \n",
    "        #page = s.get(url, timeout=30)        \n",
    "        #tree = html.fromstring(page.content)\n",
    "        try:\n",
    "            pageattrs = tree.xpath('//div[@class=\"mapAndAttrs\"]/p[@class=\"attrgroup\"]/span/b')\n",
    "        except:\n",
    "            pageattrs = []\n",
    "\n",
    "        return pageattrs\n",
    "    \n",
    "        if 'private room' in pageattrs:\n",
    "            private_room = True \n",
    "   \n",
    "        if 'private bath' in pageattrs:\n",
    "            private_bath = True\n",
    "            \n",
    "        parking=['carport','attached garage','detached garage']\n",
    "        if any(i in parking for i in pageattrs):\n",
    "            carport_or_garage = True\n",
    "\n",
    "        if 'w/d in unit' in pageattrs:\n",
    "            washer_unit = True\n",
    "            \n",
    "        washer_list=['laundry in bldg','laundry on site']\n",
    "        if any(i in washer_list for i in pageattrs):\n",
    "            washer_building = True\n",
    "      \n",
    "        return [private_room, private_bath, carport_or_garage, washer_unit, washer_building]\n",
    "            \n",
    "    \n",
    "     #def _scrapeLatLng(self, session, url, proxy=True):\n",
    "    def _scrapeLatLng(self, url):\n",
    "    \n",
    "        #s = session\n",
    "        # if proxy:\n",
    "        #     requests.packages.urllib3.disable_warnings()\n",
    "        #     authenticator = '87783015bbe2d2f900e2f8be352c414a'\n",
    "        #     proxy_str = 'http://' + authenticator + '@' +'workdistribute.charityengine.com:20000'\n",
    "        #     s.proxies = {'http': proxy_str, 'https': proxy_str}\n",
    "        #     s.auth = HTTPProxyAuth(authenticator,'') \n",
    "\n",
    "        #page = s.get(url, timeout=30)\n",
    "        \n",
    "        page = requests.get(url)\n",
    "        tree = html.fromstring(page.content)\n",
    "        #try:\n",
    "            #baths = tree.xpath('//div[@class=\"mapAndAttrs\"]/p[@class=\"attrgroup\"]/span/b')[1].text[:-2]\n",
    "        #except:\n",
    "            #baths = ''\n",
    "        map = tree.xpath('//div[@id=\"map\"]')\n",
    "\n",
    "        # Sometimes there's no location info, and no map on the page        \n",
    "        if len(map) == 0:\n",
    "            return ['', '']\n",
    "\n",
    "        map = map[0]\n",
    "        lat = map.xpath('@data-latitude')[0]\n",
    "        lng = map.xpath('@data-longitude')[0]\n",
    "        \n",
    "        \n",
    "        accuracy = map.xpath('@data-accuracy')[0]\n",
    "\n",
    "        \n",
    "        \n",
    "        #address = self._parseAddress(tree)\n",
    "        \n",
    "        #return [baths, lat, lng, accuracy, address]\n",
    "        return [lat, lng, accuracy]\n",
    "\n",
    "    def run(self):\n",
    "            #colnames = ['pid','dt','url','title','price','neighb','beds','sqft', 'baths',\n",
    "                        #'lat','lng','accuracy','address','private_room', 'private_bath', 'carport_or_garage', 'washer_unit', 'washer_building', 'body_text']\n",
    "            colnames = ['pid','dt','url','title','price','neighb','sqft',\n",
    "                        'lat','lng','accuracy','body_text','address','private_room', 'private_bath', 'carport_or_garage', 'washer_unit', 'washer_building']     #st_time = time.time()\n",
    "        \n",
    "            fname = self.out_dir + self.fname_base + '-' \\\n",
    "                + (self.ts if self.fname_ts else '') + '.csv'\n",
    "            \n",
    "\n",
    "            with open(fname, 'wb') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(colnames)\n",
    "                \n",
    "                #total_listings = 0\n",
    "                #listing_num = 0\n",
    "                #ts_skipped = 0\n",
    "                \n",
    "                    for domain in self.domains:\n",
    "                        #regionName = domain.split('//')[1].split('.craigslist')[0]\n",
    "                        regionIsComplete = False\n",
    "                        search_url = domain\n",
    "                        print(\"beginning new region\")\n",
    "                        logging.info('BEGINNING NEW REGION')\n",
    "\n",
    "       \n",
    "                \n",
    "                        while not regionIsComplete:\n",
    "\n",
    "                            logging.info(search_url)\n",
    "                            page = requests.get(search_url)\n",
    "                            #print(page.status_code)\n",
    "                            tree = html.fromstring(page.content)\n",
    "                            #return tree\n",
    "                            \n",
    "                            listings = tree.xpath('//li[@class=\"result-row\"]')\n",
    "                            #print(\"got {0} listings\".format(len(listings)))\n",
    "                            for item in listings:\n",
    "\n",
    "                                #listing_num += 1\n",
    "                                try:\n",
    "                                        row = self._parseListing(item)\n",
    "                                        item_ts = dt.strptime(row[1], '%Y-%m-%d %H:%M')\n",
    "                \n",
    "                                        if (item_ts > self.latest_ts):\n",
    "                                            # Skip this item but continue parsing search results\n",
    "                                            #ts_skipped += 1\n",
    "                                            continue\n",
    "\n",
    "                                        if (item_ts < self.earliest_ts):\n",
    "                                        # Break out of loop and move on to the next region\n",
    "                                        #if listing_num == 1:\n",
    "                                        #logging.info('NO LISTINGS BEFORE TIMESTAMP CUTOFF AT {0}'.format(str.upper(regionName)))    \n",
    "                                    #else:\n",
    "                                        #logging.info('REACHED TIMESTAMP CUTOFF')\n",
    "                                    #ts_skipped += 1\n",
    "                                            regionIsComplete = True\n",
    "                                            logging.info('REACHED TIMESTAMP CUTOFF')\n",
    "                                            break \n",
    "                    \n",
    "                                        item_url = domain.split('/search/roo')[0] + row[2]\n",
    "                                        row[2] = item_url\n",
    "                                        #item_url = domain.split('/search')[0] + tree.xpath('a/@href')[0]\n",
    "                                        logging.info(item_url)\n",
    "                                        #row += self.PageAttributes(item_url)\n",
    "                                        row += self._scrapeLatLng(item_url)\n",
    "                                        row += self.PageBodyText(item_url)\n",
    "                                        writer.writerow(row)\n",
    "                            \n",
    "                                except Exception as e:\n",
    "                                    # Skip listing if there are problems parsing it\n",
    "                                    logging.warning(\"{0}: {1}. Probably no beds/sqft info\".format(type(e).__name__, e))\n",
    "                                    continue\n",
    "                                    \n",
    "            return   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scraper = RentalListingScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning new region\n"
     ]
    }
   ],
   "source": [
    "scraper.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MUST FIGURE OUT HOW TO GET BEDROOMS\n",
    "#Took out baths from original code because baths are in different location for shared\n",
    "#What is row[2] in item_url = domain.split('/search')[0] + row[2]\n",
    "#logging not working\n",
    "#Add back in other functions\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def PageAttributes(self, session, url, proxy=True):\n",
    "    \n",
    "        s = session\n",
    "        \n",
    "        page = s.get(url, timeout=30)        \n",
    "        tree = html.fromstring(page.content)\n",
    "        try:\n",
    "            pageattrs = tree.xpath('//div[@class=\"mapAndAttrs\"]/p[@class=\"attrgroup\"]/span/b')\n",
    "        except:\n",
    "            pageattrs = []\n",
    "\n",
    "        return pageattrs\n",
    "    \n",
    "        if 'private room' in pageattrs:\n",
    "            private_room = True \n",
    "   \n",
    "        if 'private bath' in pageattrs:\n",
    "            private_bath = True\n",
    "            \n",
    "        parking=['carport','attached garage','detached garage']\n",
    "        if any(i in parking for i in pageattrs):\n",
    "            carport_or_garage = True\n",
    "\n",
    "        if 'w/d in unit' in pageattrs:\n",
    "            washer_unit = True\n",
    "            \n",
    "        washer_list=['laundry in bldg','laundry on site']\n",
    "        if any(i in washer_list for i in pageattrs):\n",
    "            washer_building = True\n",
    "      \n",
    "        return [private_room, private_bath, carport_or_garage, washer_unit, washer_building]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
